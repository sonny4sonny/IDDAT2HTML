<!DOCTYPE html>
<html lang="en">
<head>
<link rel="stylesheet" href="style.css">
  <meta charset="UTF-8">
  <title>UI Milestone – Sonny</title>
  <style>
    body, html {
      margin: 0; padding: 0;
      font-family: Arial, sans-serif;
      background: #2d3436;
      color: #f1f1f1;
    }

    h1, h2 {
      margin: 0 0 10px;
      font-weight: bold;
    }

    h1 { font-size: 28px; color: #fd79a8; }
    h2 { font-size: 20px; color: #ffeaa7; }

    main {
      max-width: 900px;
      margin: 40px auto;
      padding: 0 20px;
    }

    section {
      background: #3a3f41;
      border-radius: 8px;
      padding: 20px;
      margin-bottom: 30px;
      box-shadow: 0 4px 12px rgba(0,0,0,0.3);
    }

    section p, section li { line-height: 1.5; }

    .tag {
      display: inline-block;
      background: #fd79a8;
      color: #2d3436;
      font-size: 13px;
      font-weight: bold;
      padding: 2px 8px;
      border-radius: 4px;
      margin-right: 6px;
    }

    a { color: #8be9fd; }

    .button {
      display: inline-block;
      padding: 12px 24px;
      background: #fd79a8;
      color: #2d3436;
      border-radius: 6px;
      font-weight: bold;
      text-decoration: none;
      transition: all 0.2s;
    }
    .button:hover {
      background: #ffeaa7;
      color: #2d3436;
    }

    form label {
      display: block;
      margin: 12px 0 6px;
      font-weight: bold;
      color: #ffeaa7;
    }

    textarea {
      width: 100%;
      padding: 8px;
      border-radius: 6px;
      border: none;
      font-family: inherit;
      resize: vertical;
    }

    button[type="submit"] {
      margin-top: 15px;
      padding: 10px 20px;
      background: #fd79a8;
      color: #2d3436;
      border: none;
      border-radius: 6px;
      font-weight: bold;
      cursor: pointer;
      transition: all 0.2s;
    }
    button[type="submit"]:hover {
      background: #ffeaa7;
      color: #2d3436;
    }

    
  </style>
</head>
<body>

  <main>
    <header style="margin-bottom:30px;">
      <h1>UI Milestone</h1>
      <p><strong> Interface Design and Development</strong><br>
      Hamish Foy | ID: S4109866</p>
    </header>

    <section>
      <h2>Live Prototype</h2>
      <p><a href="https://iddmilestone1.netlify.app/" target="_blank" class="button">Open Prototype</a></p>
    </section>
 
    <section>
  <h2>Conceptual Response</h2>
  <span class="tag">Prompt: Novel Input</span>
  <span class="tag">Technique: Location</span>

  <p>
    This project is an audio-visualiser controlled by location and user input.
    Day/night data from the Open-Meteo API determines mood (light or dark),
    changing both UI colour and synth timbre, while longitude sets tempo with a
    displayed BPM. A persistent synth loop and pulsing kick form the core sound.
    User drag across the screen refines this output: horizontal drag selects
    notes from a scale, while vertical drag controls note density.
  </p>

  <p>
    This avoids traditional sliders or menus within the UI in pursuit of
    intuitive, natural interaction. The project demonstrates how
    environmental data (location and time) and gesture can shape audiovisual
    feedback without relying on 'conventional' controls.
  </p>

  <h3>Information Architecture</h3>
  <p>
    The interface is deliberately minimal: a full-screen non-scrollable canvas, a pulsing circle,
    and a HUD overlay in the top corner. This separation ensures that sound, visual,
    and data layers never distract from eachother or the UX. Users see their geolocation,
    BPM, and mode clearly presented, while all interactive input take place directly
    on the visualiser.
  </p>

  <h3>Mapping</h3>
  <p>
    Each input maps transparently to an output: day/night → mode (light/dark);
    longitude → tempo (BPM); drag X → pitch; drag Y → density. These mappings
    are both functional and aesthetic, using environmental and embodied data to
    reinforce novel input. The data is shown live in the HUD
    so users can understand how their environment shape the system.
  </p>

  <h3>Feedback</h3>
  <p>
    Feedback is both auditory and visual. The kick drum and synth loops give
    constant rhythm, while the circle pulses with the beat for
    visual confirmation. The HUD readout provides immediate textual feedback on BPM,
    mode, and location data, essentially closing the 'loop' between user input and system
    response.
  </p>

  <h3>Characterisation</h3>
  <p>
    The system itself is characterised as playful and responsive; more like a
     instrumental extention rather than a passive visualiser. It has a relatively simple identity: rhythmic, pulsing, and reactive, with mood
    shifting between bright and dark depending on the user’s real-world data.
    This characterisation is important for making the interaction feel alive and tangible
    rather than mechanical and automated.
  </p>

  <p><strong>Design references:</strong><br></p>

         – <a href="http://www.patatap.com" target="_blank">Patatap</a><br>
         – <a href="https://www.newrafael.com/" target="_blank">Rafael Rozendaal</a></p>
    </section>

  <section>
  <h2>Visual Plan / Sketch</h2>
  <p>Reference sketches showing the design process from early wireframes to final product:</p>

  <div class="sketch-grid">
    <div>
      <img src="images/sketch1.png" alt="Early wireframe: tap to start">
      <p class="caption">Sketch 1: Initial concept</p>
    </div>
    <div>
      <img src="images/sketch2.png" alt="Wireframe with drag controls">
      <p class="caption">Sketch 2: Spatial mapping</p>
    </div>
    <div>
      <img src="images/sketch3.png" alt="Wireframe with BPM box">
      <p class="caption">Sketch 3: Dark mode concept</p>
    </div>
    <div>
      <img src="images/sketch4.png" alt="Final refined visualiser">
      <p class="caption">Sketch 4: Light mode concept</p>
    </div>
  </div>
</section>
</section>
    </section>

    <section>
      <h2>Technical Approach</h2>
      <p>Advanced method: <code>navigator.geolocation.getCurrentPosition</code>.</p>

<svg viewBox="0 0 1120 520" width="100%" role="img" aria-label="System map: Location to Mode & Tempo; Drag to Pitch & Density; Engine to Visual and Sound">
  <style>
    .bg { fill:#3a3f41; }
    .box { fill:#2d3436; stroke:#555; stroke-width:2; rx:14; }
    .small { fill:#2d3436; stroke:#555; stroke-width:2; rx:10; }
    .title { font: 700 18px Arial, sans-serif; fill:#ffeaa7; }
    .label { font: 600 16px Arial, sans-serif; fill:#f1f1f1; }
    .meta  { font: 500 14px Arial, sans-serif; fill:#cfcfcf; }
    .tag   { fill:#fd79a8; }
    .tagTxt{ font: 700 13px Arial, sans-serif; fill:#2d3436; }
    .arrow { stroke:#8be9fd; stroke-width:3; marker-end:url(#arrowHead); fill:none; }
  </style>

  <defs>
    <marker id="arrowHead" markerWidth="10" markerHeight="10" refX="8" refY="3" orient="auto">
      <path d="M0,0 L8,3 L0,6 Z" fill="#8be9fd"></path>
    </marker>
  </defs>

  <rect class="bg" x="0" y="0" width="1120" height="520" rx="18"></rect>

  <text class="title" x="90"  y="50">INPUT</text>
  <text class="title" x="430" y="50">MAPPING</text>
  <text class="title" x="760" y="50">ENGINE</text>
  <text class="title" x="940" y="50">OUTPUT</text>

  <rect class="small" x="60" y="80" width="220" height="70"></rect>
  <text class="label" x="86" y="122">Tap to Start (audio + locate)</text>

  <rect class="box" x="60" y="170" width="220" height="120"></rect>
  <text class="label" x="86" y="205">Geolocation</text>
  <text class="meta"  x="86" y="232">navigator.geolocation</text>

  <rect class="tag" x="230" y="182" width="40" height="18" rx="4"></rect>
  <text class="tagTxt" x="235" y="195">API</text>

  <rect class="box" x="60" y="310" width="220" height="140"></rect>
  <text class="label" x="86" y="345">Drag on Canvas</text>
  <text class="meta"  x="86" y="372">X → position</text>
  <text class="meta"  x="86" y="394">Y → position</text>

  <rect class="box" x="320" y="170" width="280" height="120"></rect>
  <text class="label" x="346" y="205">Location Mapping</text>
  <text class="meta"  x="346" y="232">Latitude → Mode (bright/dark)</text>
  <text class="meta"  x="346" y="254">Longitude → Tempo</text>

  <rect class="box" x="320" y="310" width="280" height="140"></rect>
  <text class="label" x="346" y="345">Gesture Mapping</text>
  <text class="meta"  x="346" y="372">X → Change on scale (major/minor)</text>
  <text class="meta"  x="346" y="394">Y → Note density (probability)</text>

  <rect class="box" x="620" y="220" width="240" height="160"></rect>
  <text class="label" x="646" y="255">Tone.js </text>
  <text class="meta"  x="646" y="282">Synth</text>
  <text class="meta"  x="646" y="304">Kick (half-notes)</text>
  <text class="meta"  x="646" y="326">Melody Loop (8th notes)</text>

 
  <rect class="box" x="890" y="170" width="200" height="100"></rect>
  <text class="label" x="906" y="205">Visual</text>
  <text class="meta"  x="906" y="232">Circle pulse + HUD</text>

  <rect class="box" x="890" y="300" width="200" height="100"></rect>
  <text class="label" x="906" y="335">Sound</text>
  <text class="meta"  x="906" y="362">Mode, Tempo, Pitch, Density</text>

  <path class="arrow" d="M280,115 L320,115 L320,230"></path>
  <path class="arrow" d="M280,115 L300,115 L300,380 L320,380"></path>

  <path class="arrow" d="M280,230 L320,230"></path>

  <path class="arrow" d="M280,380 L320,380"></path>

  <path class="arrow" d="M600,230 L620,230"></path>
  <path class="arrow" d="M600,380 L620,380"></path>

  <path class="arrow" d="M860,260 L890,220"></path>
  <path class="arrow" d="M860,340 L890,350"></path>
</svg>
    </section>


    <section>
      <h2>Peer Feedback Form</h2>
      <form action="https://formspree.io/f/xkgvelne" method="POST">
        <label for="clarity">1.Was the connection between location data (mode/tempo) and the output clear during use?</label>
        <textarea id="clarity" name="clarity" rows="2" required></textarea>


        <label for="overall">2. Did the combination of sound and visuals feel engaging, or would you suggest additional features or changes?</label>
        <textarea id="overall" name="overall" rows="3" required></textarea>

        <button type="submit">Submit Feedback</button>
      </form>
    </section>

     <section>
      <section class="card">
  <h2>Feedback Summary</h2>

  <h3>Clarity of location mapping</h3>
  <p>
    From testing, I found that many users enjoyed the sound and visuals, but struggled 
    to understand exactly what the location data was doing. Because testing occurred in only 
    one physical location, the effect of latitude/longitude on tempo and mode was not obvious. 
    Several people mentioned they were unsure if “location” referred to their physical device 
    or their drag input.
  </p>

  <h3>Engagement of audiovisual interaction</h3>
  <p>
    Almost all peers described the sound as “engaging,” “fun,” or “cool,” noting that the 
     kick and pulsing circle made the system feel dynamic. Suggestions included more 
    colour variation or even additional melodic elements.
  </p>

  <h2>Response to Feedback</h2>
  <p>
    To address the first issue, I revised the way location data influences the system. 
    Originally, latitude controlled bright/dark mood, but this mapping felt abstract and 
    was not legible for testing in one place. Following testing, I shifted to using the 
    free Open-Meteo API’s day/night data to control light/dark modes. This makes the link 
    between environment and audiovisual output much more direct: if it is day at the user’s 
    current location, the visualiser runs in light mode; if it is night, it runs in dark mode. 
    The HUD now explicitly displays this state, so users can easily see how their environment 
    is being mapped.
  </p>
  <p>
    On engagement, I kept the strong rhythmic foundation that users enjoyed, while leaving room 
    for future extensions by AT3 submission. The circle continues to pulse in sync with the beat, 
    but modes are now controlled by the day/night data, enhancing visual variation and hopefully user 
    engagement. A BPM tracker was also added to the HUD to make tempo changes clearer. While I didn't 
     add a second instrument at this stage (to avoid overworking myself or leaving it 
    scrappy by Friday’s submission), the reflection process has definetly highlighted it as a potential 
    addition for AT3’s outcome.
  </p>

  <h2>Summary</h2>
  <p>
    Peer testing confirmed the prototype was already engaging, but confirmed that the clarity of 
    location input to output mapping needed refinement. By tying the light/dark modes directly to real-world 
    day/night data at the user’s location, the project now has a more intuitive and defined 
    relationship between environment and audiovisual output.
  </p>
</section>
    </section>


    <section>
      <h2>Attributions</h2>
      <ul>
        <li>Tone.js — <a href="https://tonejs.github.io/" target="_blank">https://tonejs.github.io/</a></li>
        <li>MDN Web Docs — <a href="https://developer.mozilla.org/" target="_blank">https://developer.mozilla.org/</a></li>
      </ul>
    </section>
  </main>

</body>
</html>